{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from stemming.porter2 import stem\n",
    "from textblob import TextBlob\n",
    "import sys  \n",
    "import math\n",
    "from collections import Counter\n",
    "import os\n",
    "import operator\n",
    "import re\n",
    "import scipy\n",
    "from string import punctuation\n",
    "import numpy\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "import random\n",
    "# fix UTF8 errors\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "techList = [\"startup\", \"entrepreneurship\", \"reconstruct\", \"wind\", \"talent\", \"prototype\", \"mentorship\", \"hydro\",\n",
    "\"ventur\", \"electrif\", \"internship\", \"tech\", \"technic\", \"technolog\", \"incub\", \"batch\", \"internet\", \"lab\", \"manufacturer\", \n",
    "\"solar\", \"air\", \"renewab\", \"acceler\", \"hub\", \"rd\", \"invest\", \"car\", \"renew\", \"mobil\", \"research\", \"network\", \"onlin\", \"pilot\", \n",
    "\"msme\", \"mfis\", \"it\", \"e-govern\", \"om\", \"excel\", \"surveil\", \"ict\", \"innov\", \"energi\", \"experi\", \"test\", \"progress\", \"introduc\"]\n",
    "\n",
    "peopleList = [\"governor\", \"mentorship\", \"angel\",\"trader\",\"manufacturer\",\"mentor\",\"entrepreneur\",\"secretariat\",\n",
    "\"internship\",\"company\",\"graduate\",\"trainer\",\"supplier\",\"auditor\",\"youth\",\"master\",\"member\",\"team\",\"exchange\",\"coach\",\"committee\",\"forum\",\n",
    "\"institut\",\"institution\",\"council\",\"offici\",\"student\",\"collaborate\",\"discuss\",\"faculti\",\"workshop\",\"lab\",\"centre\",\"agency\",\"consult\",\n",
    "\"particip\",\"assist\",\"student\",\"depend\",\"societi\",\"dissemin\",\"associ\",\"outreach\",\"guid\",\"person\",\"share\",\"competit\",\"firm\",\n",
    "\"supervis\",\"author\",\"engag\"]\n",
    "\n",
    "locationList = [\"countri\", \"market\", \"counti\", \"citi\", \"govern\", \"sector\", \"region\", \"state\", \"network\", \"agenc\", \"municip\", \"infrastructure\",\n",
    "\"intern\", \"local\", \"urban\", \"rural\"]\n",
    "\n",
    "modifierList = [\"minimum\", \"sound\",\"resilient\",\"simple\",\"solid\",\"good\",\"viable\",\"proper\",\"light\",\"firm\",\"minor\",\"suitable\",\"feasible\",\n",
    "\"veri\",\"full\",\"least\",\"small\",\"averag\",\"specif\",\"potenti\",\"onli\",\"possibl\",\"more\",\"additional\",\"main\",\"over\",\"basic\",\"high\",\"direct\",\"limit\",\"various\",\"least\"]\n",
    "actVerbList = [\"implement\",\"plan\",\"oper\",\"evalu\",\"work\",\"build\",\"transport\",\"perform\",\"integr\",\n",
    "\"strengthen\",\"coordin\",\"construct\",\"impact\",\"audit\",\"exercise\",\"design\",\"procur\",\"establish\",\"conduct\",\"start\"]\n",
    "\n",
    "timeList =[\"year\",\"month\",\"week\", \"day\",\"hour\",\"time\",\"spring\",\"summer\",\"fall\",\"winter\",\"january\",\"february\",\"march\",\n",
    "\"april\", \"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\", \"annual\", \"yearly\", \"monthly\", \"weekly\", \"daily\", \"biweekly\",\n",
    "\"last\", \"first\", \"near\", \"immediate\", \"quick\", \"earli\", \"gradual\", \"rapid\", \"initi\", \"futur\", \"regular\", \"second\", \"third\", \"new\", \"frequent\"]\n",
    "\n",
    "financeList = [\"financ\",\"cost\",\"procur\",\"grant\",\"fund\",\"financi\",\"loan\",\"alloc\",\"revenue\",\"menu\",\"bid\",\"spend\",\"economi\",\n",
    "\"equity\",\"lend\",\"invest\",\"ida\",\"capit\",\"account\",\"credit\"]\n",
    "\n",
    "jargonList = [\"support\",\"develop\",\"manag\",\"provid\",\"improv\",\"inform\",\"monitor\",\"process\",\"requir\",\"assess\",\"result\",\"effect\",\n",
    "\"prepar\",\"report\",\"target\",\"identifi\",\"strategi\",\"resource\",\"key\",\"ensure\",\"data\",\"measur\",\"staff\",\"organ\",\"review\",\n",
    "\"facil\",\"issue\",\"carry\",\"approach\",\"expect\",\"indic\",\"effici\",\"practic\",\"procedur\",\"detail\",\"respons\"]\n",
    "\n",
    "listOfWordLists = [[\"actVerb\", actVerbList], [\"modifier\", modifierList], [\"location\", locationList], [\"finance\", financeList], [\"people\", peopleList], [\"tech\", techList],  [\"time\", timeList], [\"jargon\", jargonList]]\n",
    "#listOfWordLists = [[\"time\", timeWords], [\"risk\", riskWords], [\"customer\", customerWords], [\"extreme\", extremeWords], [\"technology\", technologyWords],\n",
    "#[\"innovative\", innovativeWords], [\"implementation\", implementationWords]]\n",
    "\n",
    "# get sentence length, punctuation, section length, part-of-speech, etc.\n",
    "def getPunctAndPOSFeatures(fileStr):\n",
    "\t#pos\n",
    "\t#https://www.garysieling.com/blog/part-of-speech-tagging-nltk-vs-stanford-nlp\n",
    "\ttext = nltk.word_tokenize(fileStr)\n",
    "\ttagged = nltk.pos_tag(text)\n",
    "\tpos = Counter([y[1] for y in tagged])\n",
    "\tpos = Counter({k: v / float(len(tagged)) for k, v in pos.items()})\n",
    "\t#punct\n",
    "\tsentences = sentenceTokenizer.tokenize(fileStr)\n",
    "\twordsInSentences = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "\twordsPerSentence = float(sum(wordsInSentences))/len(wordsInSentences)\n",
    "\twordsInSection = sum(wordsInSentences)\n",
    "\tpunctuation_counts =  {k:v/float(sum(wordsInSentences)) for k, v in Counter(fileStr).iteritems() if k in punctuation}\n",
    "\treturn wordsPerSentence, wordsInSection, punctuation_counts, pos\n",
    "\n",
    "def countAbbrevs(fileStr):\n",
    "\t#tokenize words\n",
    "\twords = fileStr.split()\n",
    "\t#find all caps words\n",
    "\tabbrevs = [x for x in words if x.isupper() and not any(char.isdigit() for char in x) and len(x) > 1]\n",
    "\t#num abbrevs, num unique abbrevs\n",
    "\treturn len(abbrevs), len(set(abbrevs))\n",
    "\n",
    "\n",
    "# count frequencies of different categories of words (time-based, risk-based) and compute sentiment scores\n",
    "def countWords(fileStr):\n",
    "\t#remove punctuation\n",
    "\tfileStr = fileStr.translate(None, string.punctuation)\n",
    "\t#lowercase all\n",
    "\tfileStr = fileStr.lower()\n",
    "\t#get rid of troublesome characters\n",
    "\tfileStr = unicode(fileStr, errors='ignore')\n",
    "\t####print TextBlob(fileStr).sentiment\n",
    "\t#tokenize words\n",
    "\tmyDict = fileStr.split()\n",
    "\t#stem words\n",
    "\tmyDict = [stem(word) for word in myDict]\n",
    "\tallCatsFreqs = {}\n",
    "\t#stem words in matching wordList\n",
    "\tfor category in listOfWordLists:\n",
    "\t\tcatWordFreqs = {}\n",
    "\t\twordList = [stem(word) for word in category[1]]\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tcatWordFreqs[word] = float(myDict.count(word)) / len(myDict)\n",
    "\t\t#print catWordFreqs\n",
    "\t\tallCatsFreqs[category[0]] = catWordFreqs\n",
    "\treturn allCatsFreqs\n",
    "\n",
    "def countWordsAndCompareToBaseline(fileStr, fileType, baselines):\n",
    "\t#tokenize words\n",
    "\tmyDict = fileStr.split()\n",
    "\t#stem words\n",
    "\tmyDict = [stem(word) for word in myDict]\n",
    "\tallCatsFreqs = {}\n",
    "\t#stem words in matching wordList\n",
    "\tfor category in listOfWordLists:\n",
    "\t\tcatWordFreqs = {}\n",
    "\t\twordList = [stem(word) for word in category[1]]\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tif fileType in baselines[category[0]][word]:\n",
    "\t\t\t\tbaselineWord = baselines[category[0]][word][fileType]\n",
    "\t\t\t\tif baselineWord != 0.0:\n",
    "\t\t\t\t\tcatWordFreqs[word] = (float(myDict.count(word)) / len(myDict)) / baselineWord\n",
    "\t\t\t#catWordFreqs[word] = (float(myDict.count(word)) / len(myDict))\n",
    "\t\t#print fileType, category[0], statistics.mean(catWordFreqs) if len(catWordFreqs) > 0 else None\n",
    "\t\tallCatsFreqs[category[0]] = catWordFreqs\n",
    "\treturn allCatsFreqs\n",
    "\n",
    "def safeCheck(dic, word):\n",
    "\treturn dic[word] if word in dic else 0.0\n",
    "\n",
    "def getFeaturesForModel(features):\n",
    "\treturn [[x['numUniqueAbbrevs'], x['numAbbrevs'], x['sentiment'].polarity, x['sentiment'].subjectivity, x['wordsPerSentence'], x['wordsInSection'], safeCheck(x['punctuationCounts'], '$'), \n",
    "\tsafeCheck(x['punctuationCounts'], ')'), safeCheck(x['punctuationCounts'], '-'),safeCheck(x['punctuationCounts'], ','), \n",
    "\tsafeCheck(x['punctuationCounts'], ':'), x['actVerb'], x['modifier'], x['location'], x['finance'], x['people'], x['tech'], x['time'], x['jargon'],\n",
    "\t(safeCheck(x['pos'], 'NN') + safeCheck(x['pos'], 'NNP') + safeCheck(x['pos'], 'NNS')), safeCheck(x['pos'], 'IN'), safeCheck(x['pos'], 'JJ'),\n",
    "\tsafeCheck(x['pos'], 'DT'), safeCheck(x['pos'], 'CC'), safeCheck(x['pos'], 'MD'), safeCheck(x['pos'], 'RB'), \n",
    "\t(safeCheck(x['pos'], 'VB') + safeCheck(x['pos'], 'VBN') + safeCheck(x['pos'], 'VBG') + safeCheck(x['pos'], 'VBZ') + safeCheck(x['pos'], 'VBP')),\n",
    "\tsafeCheck(x['pos'], 'CD'), safeCheck(x['pos'], 'TO')] for x in features]\n",
    "\n",
    "def main(innovPath, nonInnovPath):\n",
    "\tpcodes = [line.rstrip('\\n').split('\\t') for line in open('pcodes.txt')]\n",
    "\tcats = [x[0] for x in listOfWordLists]\n",
    "\n",
    "\t# get baseline\n",
    "\tbaselineDicts = transformDocToDict(\"FULL/components/non_innov_non_innov_TTL/baseline\", None, countWords)\n",
    "\tbaselineDictsWithFileTypes = [[projInfo[7].lower()] + processed for processed in baselineDicts for projInfo in pcodes if processed[0]==projInfo[0]]\n",
    "\tbaselines = computeAvgWordFreqOverDict(baselineDictsWithFileTypes)\n",
    "\n",
    "\t### EVALUATE DICTIONARIES\n",
    "\tprint \"INNOV\"\n",
    "\tevaluate_innov = evaluateDocs(\"FULL/components/innov/\", pcodes, baselines)\n",
    "\tinnov_long = [x for x in evaluate_innov if x['fileType'] == \"long\" ]\n",
    "\t\t\n",
    "\t#non innov projs from non innov ttls!\n",
    "\tprint \"NON-INNOV\"\n",
    "\tevaluate_non_innov = evaluateDocs(\"FULL/components/non_innov_non_innov_TTL/test\", pcodes, baselines)\n",
    "\tnon_innov_long = [x for x in evaluate_non_innov if x['fileType'] == \"long\" ]\n",
    "\n",
    "\tfrom sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\tlong_features_train = getFeaturesForModel(innov_long[0:int(0.5*len(innov_long))]) + getFeaturesForModel(non_innov_long[0:int(0.5*len(non_innov_long))])\n",
    "\tlong_features_test = getFeaturesForModel(innov_long[int(0.5*len(innov_long)):]) + getFeaturesForModel(non_innov_long[int(0.5*len(non_innov_long)):])\n",
    "\n",
    "\tlong_labels_train = [1.0] * int(0.5*len(innov_long)) + [0.0] * int(0.5*len(non_innov_long))\n",
    "\tlong_labels_test = [1.0] * (len(innov_long) - int(0.5*len(innov_long))) + [0.0] * (len(non_innov_long) - int(0.5*len(non_innov_long)))\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel = model.fit(long_features_train, long_labels_train)\n",
    "\tprediction_lr = model.predict(long_features_test)\n",
    "\tprint(metrics.classification_report(long_labels_test, prediction_lr))\n",
    "\tmetrics.accuracy_score(prediction_lr, long_labels_test)\n",
    "\t# accuracy is 0.68\n",
    "\n",
    "\t# compare innov with non-innov\n",
    "\t# long only\n",
    "\tfor category in cats:\n",
    "\t\tprint category\n",
    "\t\tinnovMeans = [statistics.mean(x[category]) for x in innov_long]\n",
    "\t\tcappedInnovMeans = removeAndCapOutliers(innovMeans)\n",
    "\t\tnonInnovMeans = [statistics.mean(x[category]) for x in non_innov_long]\n",
    "\t\tcappedNonInnovMeans = removeAndCapOutliers(nonInnovMeans)\n",
    "\t\tprint cappedInnovMeans, cappedNonInnovMeans\n",
    "\t\tprint statistics.mean(cappedInnovMeans), statistics.mean(cappedNonInnovMeans)\n",
    "\t\tprint scipy.stats.ttest_ind(cappedInnovMeans, cappedNonInnovMeans, axis = 0, equal_var=False)\n",
    "\t\tprint \"\\n \\n \\n\"\n",
    "\n",
    "\t####non innov projs from innov TTLs\n",
    "\tprint \"NON-INNOV - innov ttls\"\n",
    "\tevaluate_non_innov_innov_ttl = transformDocToDict(\"FULL/components/non_innov_innov_TTL/\", None, countWordsAndCompareToBaseline)\n",
    "\tfor category in cats:\n",
    "\t\tdocMeans = [statistics.mean(x[category]) for x in evaluate_non_innov_innov_ttl]\n",
    "\t\tprint category\n",
    "\t\tprint docMeans\n",
    "\t\tprint statistics.mean(docMeans)\n",
    "\n",
    "def transformDocToDict(inputDir, extractFn = None, processFn = None):\n",
    "\tlistOfDicts = []\n",
    "\tlistOfFailedExtractions = []\n",
    "\tfor filename in os.listdir(inputDir):\n",
    "\t\tprint filename\n",
    "\t\tf = open(inputDir + \"/\" + filename, 'r')\n",
    "\t\tfileStr =  f.read().lower()\n",
    "\t\t# find detailed project description\n",
    "\t\t# find last instance of desired annex\n",
    "\t\tif (extractFn != None):\n",
    "\t\t\tstartInd, endInd = extractFn(fileStr)\n",
    "\t\t\tif startInd < 0 or endInd < 0 or (endInd - startInd) < 500:\n",
    "\t\t\t\tprint filename, \"not found!\"\n",
    "\t\t\t\tlistOfFailedExtractions.append(filename)\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tsection = fileStr[startInd:endInd]\n",
    "\t\t\tif \"........\" in section: \n",
    "\t\t\t\tprint filename, \"not found!\"\n",
    "\t\t\t\tlistOfFailedExtractions.append(filename)\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint filename, \"found!\"\n",
    "\t\telse:\n",
    "\t\t\tsection = fileStr\n",
    "\t\tdocDict = processFn(section)\n",
    "\t\tlistOfDicts.append([filename[0:7], docDict])\n",
    "\treturn listOfDicts\n",
    "\n",
    "def evaluateDocs(inputDir, pcodes, baselines, getDictFeatures = True):\n",
    "\tdocFeatures = []\n",
    "\tsentenceTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\tfor filename in os.listdir(inputDir):\n",
    "\t\tfeatures = {'name': filename[0:7]}\n",
    "\t\tprint filename\n",
    "\t\tf = open(inputDir + \"/\" + filename, 'r')\n",
    "\t\tfileStr =  f.read()\n",
    "\t\t#get rid of troublesome characters\n",
    "\t\tfileStr = unicode(fileStr, errors='ignore')\n",
    "\t\t#\n",
    "\t\t#get punct features\n",
    "\t\twordsPerSentence, wordsInSection, punctuation_counts, pos = getPunctAndPOSFeatures(fileStr)\n",
    "\t\tfeatures['wordsPerSentence'] = wordsPerSentence\n",
    "\t\tfeatures['wordsInSection'] = wordsInSection\n",
    "\t\tfeatures['punctuationCounts'] = punctuation_counts\n",
    "\t\tfeatures['pos'] = pos\n",
    "\t\t#\n",
    "\t\t#remove punctuation\n",
    "\t\texclude = set(string.punctuation)\n",
    "\t\tfileStr = ''.join(ch for ch in fileStr if ch not in exclude)\n",
    "\t\t#\n",
    "\t\t# count abbrevs\n",
    "\t\tnumAbbrevs, numUniqueAbbrevs = countAbbrevs(fileStr)\n",
    "\t\tfeatures['numAbbrevs'] = numAbbrevs\n",
    "\t\tfeatures['numUniqueAbbrevs'] = numUniqueAbbrevs\n",
    "\t\t#\n",
    "\t\t# lower case \n",
    "\t\tfileStr = fileStr.lower()\n",
    "\t\t# get sentiment\n",
    "\t\tfeatures['sentiment']  = TextBlob(fileStr).sentiment\n",
    "\t\t#\n",
    "\t\tfileType = [x[7] for x in pcodes if x[0] == filename[0:7]][0].lower()\n",
    "\t\tif getDictFeatures:\n",
    "\t\t\tif fileType == \"pad\" or fileType == \"project paper\":\n",
    "\t\t\t\tfeatures['fileType'] = \"long\"\n",
    "\t\t\t\tfeatures['dictFeatures'] = countWordsAndCompareToBaseline(fileStr, \"long\", baselines)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeatures['fileType'] = \"short\"\n",
    "\t\t\t\tfeatures['dictFeatures'] = countWordsAndCompareToBaseline(fileStr, \"short\", baselines)\n",
    "\t\tfor cat, catDict in features['dictFeatures'].iteritems():\n",
    "\t\t\t#print catDict\n",
    "\t\t\tif len(catDict.values()) > 0:\n",
    "\t\t\t\tcatMean = sum(catDict.values())/float(len(catDict.values()))\n",
    "\t\t\t\tfeatures[cat] = catMean\n",
    "\t\tdocFeatures.append(features)\n",
    "\treturn docFeatures\n",
    "\n",
    "#use this method to compute baselines for each category, filetype pair (e.g. 'time', \"LONG\")\n",
    "#filetype options are LONG, SHORT\n",
    "#PAD includes PAD and proj paper\n",
    "#non-PAD includes PID, PCN, and misc.\n",
    "def computeAvgWordFreqOverDict(listOfDicts):\n",
    "\tdict0 = listOfDicts[0][2]\n",
    "\tallCatMeans = {}\n",
    "\tfor k, v in dict0.iteritems():\n",
    "\t\toneCatMean = {}\n",
    "\t\tallDictsForCat = [[x[2][k], x[0]] for x in listOfDicts]\n",
    "\t\tlongWordMeans = []\n",
    "\t\tshortWordMeans = []\n",
    "\t\tfor kWord, vWord in allDictsForCat[0][0].iteritems():\n",
    "\t\t\tfor currDict in allDictsForCat:\n",
    "\t\t\t\tif currDict[1] == \"pad\" or currDict[1] == \"project paper\":\n",
    "\t\t\t\t\tlongWordMeans.append(currDict[0][kWord])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tshortWordMeans.append(currDict[0][kWord])\n",
    "\t\t\tmeanLongWordFreq = sum(longWordMeans)/len(longWordMeans)\n",
    "\t\t\t#meanShortWordFreq = sum(shortWordMeans)/len(shortWordMeans)\n",
    "\t\t\toneCatMean[kWord] = {'long': meanLongWordFreq}\n",
    "\t\tallCatMeans[k] = oneCatMean\n",
    "\treturn allCatMeans\n",
    "\n",
    "#postprocessing\n",
    "def removeAndCapOutliers(dataList):\n",
    "\tmean = statistics.mean(dataList)\n",
    "\tsd = statistics.stdev(dataList)\n",
    "\tupperCutoff = mean + 3 * sd\n",
    "\tlowerCutoff = mean - 3*sd\n",
    "\tprint upperCutoff, lowerCutoff\n",
    "\tprint [x for x in dataList if x > upperCutoff or x < lowerCutoff]\n",
    "\tfinalList = []\n",
    "\tfor x in dataList:\n",
    "\t\tif x > upperCutoff:\n",
    "\t\t\tfinalList.append(upperCutoff)\n",
    "\t\telif x < lowerCutoff:\n",
    "\t\t\tfinalList.append(lowerCutofff)\n",
    "\t\telse:\n",
    "\t\t\tfinalList.append(x)\n",
    "\treturn finalList\n",
    "\n",
    "\n",
    "### THE FOLLOWING METHODS HELP GENERATE WORD CANDIDATES FOR DICTS\n",
    "def combinedDicts(listOfDicts):\n",
    "\tprint \"combining document dictionaries\"\n",
    "\tfullDict = {}\n",
    "\tfor currDict in listOfDicts:\n",
    "\t\tfor k, v in currDict.iteritems():\n",
    "\t\t\tif k in fullDict:\n",
    "\t\t\t\tfullDict[k].append(v)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfullDict[k] = [v]\n",
    "\tfullDict = {k: v + [0.0] * (len(listOfDicts) - len(v)) for k, v in fullDict.items()}\n",
    "\treturn fullDict\n",
    "\t\n",
    "\t# remove words in less than N docs\n",
    "\t# repeat for set of non-innovative projeccts\n",
    "def compareDicts(innovDict, nonInnovDict):\n",
    "\t# do t-tests to compare freqs of of any word that appears at least once in innov Dict\n",
    "\t# get # of docs each corpus\n",
    "\tprint \"comparing full dictionaries\"\n",
    "\tnumDocsNonInnov = len(nonInnovDict['the'])\n",
    "\tnumDocsInnov = len(innovDict['the'])\n",
    "\tcompareDictResults = {}\n",
    "\tpowerCalc = TTestIndPower()\n",
    "\tfor k, v in innovDict.iteritems():\n",
    "\t\tinnovFreqs = innovDict[k]\n",
    "\t\tif k in nonInnovDict:\n",
    "\t\t\tnonInnovFreqs = nonInnovDict[k]\n",
    "\t\telse:\n",
    "\t\t\tnonInnovFreqs = [0.0] * numDocsNonInnov\n",
    "\t\t# get max freqs\n",
    "\t\tmaxInnovFreq = max(innovFreqs)\n",
    "\t\tmaxNonInnovFreq = max(nonInnovFreqs)\n",
    "\t\t# t test, 2 sample unequal variance\n",
    "\t\tt_test = scipy.stats.ttest_ind(innovFreqs, nonInnovFreqs, axis=0, equal_var=False)\n",
    "\t\t# power calculation, 2 sample t test with pooled std. get minimal detectable effect\n",
    "\t\tmde = powerCalc.solve_power(effect_size=None, nobs1=numDocsInnov, alpha=0.05, power=0.8, ratio= float(numDocsNonInnov)/numDocsInnov, alternative='two-sided') * pooledStd(innovFreqs, nonInnovFreqs)\n",
    "\t\tcompareDictResults[k] = {'meanInnovWordFreq': sum(innovFreqs) / len(innovFreqs), 'fracOfInnovDocsAppearing': sum(x > 0 for x in innovFreqs) / (0.0 + numDocsInnov), \n",
    "\t\t\t\t\t\t\t\t'meanNonInnovWordFreq': sum(nonInnovFreqs) / len(nonInnovFreqs), 'fracOfNonInnovDocsAppearing': sum(x > 0 for x in nonInnovFreqs) / (0.0 + numDocsNonInnov),\n",
    "\t\t\t\t\t\t\t\t'innovMinusNonInnovFreq': sum(innovFreqs) / len(innovFreqs) - sum(nonInnovFreqs) / len(nonInnovFreqs),\n",
    "\t\t\t\t\t\t\t\t'p_val': t_test[1], 'mde': mde, 'maxInnovFreq': maxInnovFreq, 'maxNonInnovFreq': maxNonInnovFreq, 'maxInnovMinusNonInnovFreq': maxInnovFreq - maxNonInnovFreq}\n",
    "\tsorted_compare = sorted(compareDictResults.items(), key=lambda t: -1 * t[1]['meanInnovWordFreq'])\n",
    "\treturn sorted_compare\n",
    "\n",
    "def pooledStd(arr1, arr2):\n",
    "\treturn math.sqrt(((len(arr1) - 1) * numpy.var(arr1) + (len(arr2) - 1) * numpy.var(arr2)) / (len(arr1) + len(arr2) - 2))\n",
    "\n",
    "def stdev(arr):\n",
    "\tnorm = [x / sum(arr) for x in arr ]\n",
    "\ttotal = 0\n",
    "\tavg = 1.0/len(norm)\n",
    "\tfor i in norm:\n",
    "\t\ttotal += (i - avg)**2\n",
    "\ttotal = total / len(norm)\n",
    "\tprint math.sqrt(total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
