{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from stemming.porter2 import stem\n",
    "from textblob import TextBlob\n",
    "import sys  \n",
    "import math\n",
    "from collections import Counter\n",
    "import os\n",
    "import operator\n",
    "from random import shuffle\n",
    "import nltk\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "from string import punctuation\n",
    "import numpy\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "import random\n",
    "# fix UTF8 errors\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "'''\n",
    "\n",
    "The following file contains helper code for the World Bank project on understanding internal innovation.\n",
    "\n",
    "Major functionality found in this file:\n",
    "\n",
    "1. Parsing and pre-processing project documentation (unstructured text)\n",
    "\n",
    "2. Extracting text-based features from project documentation\n",
    "\n",
    "3. Computing correlations and running t-tests over project features between innovative and non-innovative documents\n",
    "\n",
    "4. Fitting a predictive model to classify projects on is_innovative based on text features\n",
    "\n",
    "5. Methods to assist in creating dictionaries based on word frequencies from a set of innovative and a set of non-innovative documents\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "techList = [\"startup\", \"entrepreneurship\", \"reconstruct\", \"wind\", \"talent\", \"prototype\", \"mentorship\", \"hydro\",\n",
    "\"ventur\", \"electrif\", \"internship\", \"tech\", \"technic\", \"technolog\", \"incub\", \"batch\", \"internet\", \"lab\", \"manufacturer\", \n",
    "\"solar\", \"air\", \"renewab\", \"acceler\", \"hub\", \"rd\", \"invest\", \"car\", \"renew\", \"mobil\", \"research\", \"network\", \"onlin\", \"pilot\", \n",
    "\"msme\", \"mfis\", \"it\", \"e-govern\", \"om\", \"excel\", \"surveil\", \"ict\", \"innov\", \"energi\", \"experi\", \"test\", \"progress\", \"introduc\"]\n",
    "\n",
    "peopleList = [\"governor\", \"mentorship\", \"angel\",\"trader\",\"manufacturer\",\"mentor\",\"entrepreneur\",\"secretariat\",\n",
    "\"internship\",\"company\",\"graduate\",\"trainer\",\"supplier\",\"auditor\",\"youth\",\"master\",\"member\",\"team\",\"exchange\",\"coach\",\"committee\",\"forum\",\n",
    "\"institut\",\"institution\",\"council\",\"offici\",\"student\",\"collaborate\",\"discuss\",\"faculti\",\"workshop\",\"lab\",\"centre\",\"agency\",\"consult\",\n",
    "\"particip\",\"assist\",\"student\",\"depend\",\"societi\",\"dissemin\",\"associ\",\"outreach\",\"guid\",\"person\",\"share\",\"competit\",\"firm\",\n",
    "\"supervis\",\"author\",\"engag\"]\n",
    "\n",
    "locationList = [\"countri\", \"market\", \"counti\", \"citi\", \"govern\", \"sector\", \"region\", \"state\", \"network\", \"agenc\", \"municip\", \"infrastructure\",\n",
    "\"intern\", \"local\", \"urban\", \"rural\"]\n",
    "\n",
    "modifierList = [\"minimum\", \"sound\",\"resilient\",\"simple\",\"solid\",\"good\",\"viable\",\"proper\",\"light\",\"firm\",\"minor\",\"suitable\",\"feasible\",\n",
    "\"veri\",\"full\",\"least\",\"small\",\"averag\",\"specif\",\"potenti\",\"onli\",\"possibl\",\"more\",\"additional\",\"main\",\"over\",\"basic\",\"high\",\"direct\",\"limit\",\"various\",\"least\"]\n",
    "actVerbList = [\"implement\",\"plan\",\"oper\",\"evalu\",\"work\",\"build\",\"transport\",\"perform\",\"integr\",\n",
    "\"strengthen\",\"coordin\",\"construct\",\"impact\",\"audit\",\"exercise\",\"design\",\"procur\",\"establish\",\"conduct\",\"start\"]\n",
    "\n",
    "timeList =[\"year\",\"month\",\"week\", \"day\",\"hour\",\"time\",\"spring\",\"summer\",\"fall\",\"winter\",\"january\",\"february\",\"march\",\n",
    "\"april\", \"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\", \"annual\", \"yearly\", \"monthly\", \"weekly\", \"daily\", \"biweekly\",\n",
    "\"last\", \"first\", \"near\", \"immediate\", \"quick\", \"earli\", \"gradual\", \"rapid\", \"initi\", \"futur\", \"regular\", \"second\", \"third\", \"new\", \"frequent\"]\n",
    "\n",
    "financeList = [\"financ\",\"cost\",\"procur\",\"grant\",\"fund\",\"financi\",\"loan\",\"alloc\",\"revenue\",\"menu\",\"bid\",\"spend\",\"economi\",\n",
    "\"equity\",\"lend\",\"invest\",\"ida\",\"capit\",\"account\",\"credit\"]\n",
    "\n",
    "jargonList = [\"support\",\"develop\",\"manag\",\"provid\",\"improv\",\"inform\",\"monitor\",\"process\",\"requir\",\"assess\",\"result\",\"effect\",\n",
    "\"prepar\",\"report\",\"target\",\"identifi\",\"strategi\",\"resource\",\"key\",\"ensure\",\"data\",\"measur\",\"staff\",\"organ\",\"review\",\n",
    "\"facil\",\"issue\",\"carry\",\"approach\",\"expect\",\"indic\",\"effici\",\"practic\",\"procedur\",\"detail\",\"respons\"]\n",
    "\n",
    "listOfWordLists = [[\"actVerb\", actVerbList], [\"modifier\", modifierList], [\"location\", locationList], [\"finance\", financeList], [\"people\", peopleList], [\"tech\", techList],  [\"time\", timeList], [\"jargon\", jargonList]]\n",
    "\n",
    "sentenceTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Get sentence length, punctuation, section length, part-of-speech, etc.\n",
    "def getPunctAndPOSFeatures(fileStr):\n",
    "\t# POD features\n",
    "\t# reference: https://www.garysieling.com/blog/part-of-speech-tagging-nltk-vs-stanford-nlp\n",
    "\ttext = nltk.word_tokenize(fileStr)\n",
    "\ttagged = nltk.pos_tag(text)\n",
    "\tpos = Counter([y[1] for y in tagged])\n",
    "\tpos = Counter({k: v / float(len(tagged)) for k, v in pos.items()})\n",
    "\t# Punctuation features\n",
    "\tsentences = sentenceTokenizer.tokenize(fileStr)\n",
    "\twordsInSentences = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "\twordsPerSentence = float(sum(wordsInSentences))/len(wordsInSentences)\n",
    "\twordsInSection = sum(wordsInSentences)\n",
    "\tpunctuation_counts =  {k:v/float(sum(wordsInSentences)) for k, v in Counter(fileStr).iteritems() if k in punctuation}\n",
    "\treturn wordsPerSentence, wordsInSection, punctuation_counts, pos\n",
    "\n",
    "# Get abbreviation features\n",
    "def countAbbrevs(fileStr):\n",
    "\t#tokenize words\n",
    "\twords = fileStr.split()\n",
    "\t#find all caps words\n",
    "\tabbrevs = [x for x in words if x.isupper() and not any(char.isdigit() for char in x) and len(x) > 1]\n",
    "\t#num abbrevs, num unique abbrevs\n",
    "\treturn len(abbrevs), len(set(abbrevs))\n",
    "\n",
    "# Extract risk features (avg risk score, # high/substantial ratings, etc.) from risk doc\n",
    "def getHMLAvg(text):\n",
    "\twords = text.split()\n",
    "\tisH = [x for x in words if x == \"h\" or x == \"high\"]\n",
    "\tisS = [x for x in words if x == \"s\" or x == \"substantial\"]\n",
    "\tisM = [x for x in words if x == \"m\" or x == \"moderate\" or x == \"medium\"]\n",
    "\tisL = [x for x in words if x == \"n\" or x == \"l\" or x == \"negligible\" or x == \"low\"]\n",
    "\trisk = len([x for x in words if x == \"risk\" or x == \"risks\"])\n",
    "\tnumRatings = len(isH + isS + isM + isL)\n",
    "\tif numRatings > 0:\n",
    "\t\tavgRating = (3.0 * len(isH) + 2.0 * len(isS) + 1.0 * len(isM))/numRatings\n",
    "\t\thigh = float(len(isH))/numRatings\n",
    "\telse:\n",
    "\t\tavgRating = 1.0\n",
    "\t\thigh = 0.0\n",
    "\tprint numRatings, avgRating, high, risk\n",
    "\treturn numRatings, avgRating, high, risk\n",
    "\n",
    "# Count frequencies for words in different dictionaries (time-based, tech, etc.)\n",
    "def countWordsForBaseline(fileStr):\n",
    "\tfileStr = fileStr.translate(None, string.punctuation)\n",
    "\tfileStr = fileStr.lower()\n",
    "\tfileStr = unicode(fileStr, errors='ignore')\n",
    "\tmyDict = fileStr.split()\n",
    "\tmyDict = [stem(word) for word in myDict]\n",
    "\tallCatsFreqs = {}\n",
    "\tfor category in listOfWordLists:\n",
    "\t\tcatWordFreqs = {}\n",
    "\t\twordList = [stem(word) for word in category[1]]\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tcatWordFreqs[word] = float(myDict.count(word)) / len(myDict)\n",
    "\t\tallCatsFreqs[category[0]] = catWordFreqs\n",
    "\treturn allCatsFreqs\n",
    "\n",
    "# Count frequencies of words in train/test documents and compare to the word frequency baselines\n",
    "def countWordsAndCompareToBaseline(fileStr, fileType, baselines):\n",
    "\tmyDict = fileStr.split()\n",
    "\tmyDict = [stem(word) for word in myDict]\n",
    "\tallCatsFreqs = {}\n",
    "\tfor category in listOfWordLists:\n",
    "\t\tcatWordFreqs = {}\n",
    "\t\twordList = [stem(word) for word in category[1]]\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tif fileType in baselines[category[0]][word]:\n",
    "\t\t\t\tbaselineWord = baselines[category[0]][word][fileType]\n",
    "\t\t\t\tif baselineWord != 0.0:\n",
    "\t\t\t\t\t# Compute ratio between document word frequency and baseline frequency of this word\n",
    "\t\t\t\t\tcatWordFreqs[word] = (float(myDict.count(word)) / len(myDict)) / baselineWord\n",
    "\t\tallCatsFreqs[category[0]] = catWordFreqs\n",
    "\treturn allCatsFreqs\n",
    "\n",
    "def safeCheck(dic, word):\n",
    "\treturn dic[word] if word in dic else 0.0\n",
    "\n",
    "def getFeaturesFromComponents(features):\n",
    "\treturn [features['numUniqueAbbrevs'], features['numAbbrevs'], features['sentiment'].polarity, features['sentiment'].subjectivity, features['wordsPerSentence'], features['wordsInSection'], safeCheck(features['punctuationCounts'], '$'), \n",
    "\tsafeCheck(features['punctuationCounts'], ')'), safeCheck(features['punctuationCounts'], '-'),safeCheck(features['punctuationCounts'], ','), \n",
    "\tsafeCheck(features['punctuationCounts'], ':'), features['actVerb'], features['modifier'], features['location'], features['finance'], features['people'], features['tech'], features['time'], features['jargon'],\n",
    "\t(safeCheck(features['pos'], 'NN') + safeCheck(features['pos'], 'NNP') + safeCheck(features['pos'], 'NNS')), safeCheck(features['pos'], 'IN'), safeCheck(features['pos'], 'JJ'),\n",
    "\tsafeCheck(features['pos'], 'DT'), safeCheck(features['pos'], 'CC'), safeCheck(features['pos'], 'MD'), safeCheck(features['pos'], 'RB'), \n",
    "\t(safeCheck(features['pos'], 'VB') + safeCheck(features['pos'], 'VBN') + safeCheck(features['pos'], 'VBG') + safeCheck(features['pos'], 'VBZ') + safeCheck(features['pos'], 'VBP')),\n",
    "\tsafeCheck(features['pos'], 'CD'), safeCheck(features['pos'], 'TO')]\n",
    "\n",
    "def getFeaturesFromPDO(features):\n",
    "\treturn [features['wordsInSection']]\n",
    "\n",
    "def getFeaturesFromRisk(features):\n",
    "\treturn [features['sentiment'].polarity, features['sentiment'].subjectivity, features['numRisks'], features['avgRisk'], features['highRisk'], features['risks']]\n",
    "\n",
    "# Generate set of features for classification modeling\n",
    "def getFullFeatures(comp, pdo, risk):\n",
    "\tfinalFeatures = []\n",
    "\tfor proj, features in comp.iteritems():\n",
    "\t\tif proj in pdo and proj in risk:\n",
    "\t\t\tfinalFeatures.append(getFeaturesFromComponents(comp) + getFeaturesFromPDO(dict2[pdo]) + getFeaturesFromRisk(dict3[risk]))\n",
    "\t# randomize order of results\n",
    "\tshuffle(finalFeatures)\n",
    "\treturn finalFeatures\n",
    "\n",
    "# Compute all features for a given document section (components, risk, PDO)\n",
    "def featureExtractionHelper(baselineDir, nonInnovDir, innovDir, pcodesForExtraction, isPdo, isRisk):\n",
    "\t# Get baseline\n",
    "\tbaselineDicts = transformDocToDict(baselineDir, None, countWordsForBaseline)\n",
    "\tbaselineDictsWithFileTypes = [[projInfo[7].lower()] + processed for processed in baselineDicts for projInfo in pcodesForExtraction if processed[0]==projInfo[0]]\n",
    "\tbaselines = computeAvgWordFreqOverDict(baselineDictsWithFileTypes)\n",
    "\t### Evaluate\n",
    "\tprint \"INNOV\"\n",
    "\tevaluate_innov = evaluateDocs(innovDir, pcodesForExtraction, baselines, True, isPdo, isRisk)\n",
    "\tinnov_long = {x['name']:x for x in evaluate_innov if x['fileType'] == \"long\"}\n",
    "\t# Non innov projs from non innov ttls\n",
    "\tprint \"NON-INNOV\"\n",
    "\tevaluate_non_innov = evaluateDocs(nonInnovDir, pcodesForExtraction, baselines, True, isPdo, isRisk)\n",
    "\tnon_innov_long =  {x['name']:x for x in evaluate_non_innov if x['fileType'] == \"long\"}\n",
    "\treturn innov_long, non_innov_long\n",
    "\n",
    "# Run t-test and compute correlation coefficients between features and is_innov\n",
    "def computeFeatSignificance(innov, non_innov):\n",
    "\tsignificance = []\n",
    "\tfor featureIndex in range(len(innov[0])):\n",
    "\t\tcapped_innov_feature_values = removeAndCapOutliers([x[featureIndex] for x in innov])\n",
    "\t\tcapped_non_innov_feature_values = removeAndCapOutliers([x[featureIndex] for x in non_innov])\n",
    "\t\tcorrCoeff = scipy.stats.pearsonr(capped_innov_feature_values + capped_non_innov_feature_values, [1.0] * len(capped_innov_feature_values) + [0.0] * len(capped_non_innov_feature_values))\n",
    "\t\tt_test= scipy.stats.ttest_ind(capped_innov_feature_values, capped_non_innov_feature_values, axis = 0, equal_var=False)\n",
    "\t\tsignificance.append([corrCoeff, t_test])\n",
    "\treturn significance\n",
    "\n",
    "def transformDocToDict(inputDir, extractFn = None, processFn = None):\n",
    "\tlistOfDicts = []\n",
    "\tfor filename in os.listdir(inputDir):\n",
    "\t\tprint filename\n",
    "\t\tf = open(inputDir + \"/\" + filename, 'r')\n",
    "\t\tsection =  f.read().lower()\n",
    "\t\tdocDict = processFn(section)\n",
    "\t\tlistOfDicts.append([filename[0:7], docDict])\n",
    "\treturn listOfDicts\n",
    "\n",
    "# Function to get all features from a document\n",
    "def evaluateDocs(inputDir, pcodes, baselines, getDictFeatures = True, isPdo = False, isRisk = False):\n",
    "\tdocFeatures = []\n",
    "\tsentenceTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\tfor filename in os.listdir(inputDir):\n",
    "\t\tfeatures = {'name': filename[0:7]}\n",
    "\t\tprint filename\n",
    "\t\tf = open(inputDir + \"/\" + filename, 'r')\n",
    "\t\tfileStr =  f.read()\n",
    "\n",
    "\t\t# Get rid of troublesome characters\n",
    "\t\tfileStr = unicode(fileStr, errors='ignore')\n",
    "\t\tif isPdo:\n",
    "\t\t\tfileStr = sentenceTokenizer.tokenize(fileStr)\n",
    "\t\t\tfileStr = fileStr[0]\n",
    "\t\twordsPerSentence, wordsInSection, punctuation_counts, pos = getPunctAndPOSFeatures(fileStr)\n",
    "\t\tfeatures['wordsPerSentence'] = wordsPerSentence\n",
    "\t\tfeatures['wordsInSection'] = wordsInSection\n",
    "\t\tfeatures['punctuationCounts'] = punctuation_counts\n",
    "\t\tfeatures['pos'] = pos\n",
    "\n",
    "\t\t# Remove punctuation\n",
    "\t\texclude = set(string.punctuation)\n",
    "\t\tfileStr = ''.join(ch for ch in fileStr if ch not in exclude)\n",
    "\n",
    "\t\t# Count abbrevs\n",
    "\t\tnumAbbrevs, numUniqueAbbrevs = countAbbrevs(fileStr)\n",
    "\t\tfeatures['numAbbrevs'] = numAbbrevs\n",
    "\t\tfeatures['numUniqueAbbrevs'] = numUniqueAbbrevs\n",
    "\n",
    "\t\t# Lower case \n",
    "\t\tfileStr = fileStr.lower()\n",
    "\n",
    "\t\t# Get sentiment\n",
    "\t\tfeatures['sentiment']  = TextBlob(fileStr).sentiment\n",
    "\t\tfileType = [x[7] for x in pcodes if x[0] == filename[0:7]][0].lower()\n",
    "\n",
    "\t\t# Dict features\n",
    "\t\tif getDictFeatures:\n",
    "\t\t\tif fileType == \"pad\" or fileType == \"project paper\":\n",
    "\t\t\t\tfeatures['fileType'] = \"long\"\n",
    "\t\t\t\tfeatures['dictFeatures'] = countWordsAndCompareToBaseline(fileStr, \"long\", baselines)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeatures['fileType'] = \"short\"\n",
    "\t\t\t\tfeatures['dictFeatures'] = countWordsAndCompareToBaseline(fileStr, \"short\", baselines)\n",
    "\t\tfor cat, catDict in features['dictFeatures'].iteritems():\n",
    "\t\t\tif len(catDict.values()) > 0:\n",
    "\t\t\t\tcatMean = sum(catDict.values())/float(len(catDict.values()))\n",
    "\t\t\t\tfeatures[cat] = catMean\n",
    "\t\tif isRisk:\n",
    "\t\t\tnumRisks, avgRisk, highRisk, risks = getHMLAvg(fileStr)\n",
    "\t\t\tfeatures['numRisks'] = numRisks\n",
    "\t\t\tfeatures['avgRisk'] = avgRisk\n",
    "\t\t\tfeatures['highRisk'] = highRisk\n",
    "\t\t\tfeatures['risks'] = risks\n",
    "\t\tdocFeatures.append(features)\n",
    "\treturn docFeatures\n",
    "\n",
    "# Use this method to compute baselines for each category, filetype pair (e.g. 'time', \"LONG\")\n",
    "# Filetype options are LONG, SHORT\n",
    "# PAD includes PAD and proj paper\n",
    "# non-PAD includes PID, PCN, and misc.\n",
    "def computeAvgWordFreqOverDict(listOfDicts):\n",
    "\tdict0 = listOfDicts[0][2]\n",
    "\tallCatMeans = {}\n",
    "\tfor k, v in dict0.iteritems():\n",
    "\t\toneCatMean = {}\n",
    "\t\tallDictsForCat = [[x[2][k], x[0]] for x in listOfDicts]\n",
    "\t\tlongWordMeans = []\n",
    "\t\tshortWordMeans = []\n",
    "\t\tfor kWord, vWord in allDictsForCat[0][0].iteritems():\n",
    "\t\t\tfor currDict in allDictsForCat:\n",
    "\t\t\t\tif currDict[1] == \"pad\" or currDict[1] == \"project paper\":\n",
    "\t\t\t\t\tlongWordMeans.append(currDict[0][kWord])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tshortWordMeans.append(currDict[0][kWord])\n",
    "\t\t\tmeanLongWordFreq = sum(longWordMeans)/len(longWordMeans)\n",
    "\t\t\tmeanShortWordFreq = sum(shortWordMeans)/len(shortWordMeans)\n",
    "\t\t\toneCatMean[kWord] = {'long': meanLongWordFreq, 'short': meanShortWordFreq}\n",
    "\t\tallCatMeans[k] = oneCatMean\n",
    "\treturn allCatMeans\n",
    "\n",
    "# Cap outliers beyond mean +- 3*SD at those thresholds\n",
    "def removeAndCapOutliers(dataList):\n",
    "\tmean = statistics.mean(dataList)\n",
    "\tsd = statistics.stdev(dataList)\n",
    "\tupperCutoff = mean + 3 * sd\n",
    "\tlowerCutoff = mean - 3*sd\n",
    "\tfinalList = []\n",
    "\tfor x in dataList:\n",
    "\t\tif x > upperCutoff:\n",
    "\t\t\tfinalList.append(upperCutoff)\n",
    "\t\telif x < lowerCutoff:\n",
    "\t\t\tfinalList.append(lowerCutoff)\n",
    "\t\telse:\n",
    "\t\t\tfinalList.append(x)\n",
    "\treturn finalList\n",
    "\n",
    "### The following methods help with PAD and PID component description extraction\n",
    "def extractPADInfo(fileStr):\n",
    "\tannexStartInd = fileStr.rfind(\": detailed project description\")\n",
    "\tif annexStartInd > 0:\n",
    "\t\tannexNumber = fileStr[fileStr.rfind(\" \", 0, annexStartInd-1) + 1 : annexStartInd]\n",
    "\t\tnextAnnexStartInd = fileStr.rfind(\"annex \" + str(int(annexNumber) + 1) + \":\")\n",
    "\telse:\n",
    "\t\tnextAnnexStartInd = -1\n",
    "\treturn annexStartInd, nextAnnexStartInd\n",
    "\n",
    "def extractPIDInfo(fileStr):\n",
    "\tdescripStartInd = max(fileStr.rfind(\". Preliminary Description\"), fileStr.rfind(\". Description\"), fileStr.rfind(\". Project Description\"),)\n",
    "\tnextDescripSectionInd = -1\n",
    "\tif descripStartInd > 0:\n",
    "\t\tdescripSection = fileStr[fileStr.rfind(\" \", 0, descripStartInd-1) + 1 : descripStartInd]\n",
    "\t\tdescripSection = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', descripSection)\n",
    "\t\tif \"\\n\" in descripSection:\n",
    "\t\t\tdescripSection = descripSection[descripSection.rfind(\"\\n\")+1:]\n",
    "\t\tif descripSection == 'III':\n",
    "\t\t\tnextDescripSectionInd = fileStr.rfind(\"IV. \")\n",
    "\t\telif descripSection == 'II': \n",
    "\t\t\tnextDescripSectionInd = fileStr.rfind(\"III. \")\n",
    "\t\telif descripSection == 'IV':\n",
    "\t\t\tnextDescripSectionInd = fileStr.rfind(\"V. \")\n",
    "\t\telif descripSection.isdigit():\n",
    "\t\t\tnextDescripSectionInd = fileStr.rfind(str(int(descripSection) + 1) + \". \") \n",
    "\t\telse:\n",
    "\t\t\tprint \"could not find next section; previous section called \", descripSection\n",
    "\treturn descripStartInd, nextDescripSectionInd\n",
    "\n",
    "# The following methods help generate and combine dictionary data in order to support the manual dictionary curation process\n",
    "def combinedDicts(listOfDicts):\n",
    "\tprint \"combining document dictionaries\"\n",
    "\tfullDict = {}\n",
    "\tfor currDict in listOfDicts:\n",
    "\t\tfor k, v in currDict.iteritems():\n",
    "\t\t\tif k in fullDict:\n",
    "\t\t\t\tfullDict[k].append(v)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfullDict[k] = [v]\n",
    "\tfullDict = {k: v + [0.0] * (len(listOfDicts) - len(v)) for k, v in fullDict.items()}\n",
    "\treturn fullDict\n",
    "\t\n",
    "def compareDicts(innovDict, nonInnovDict):\n",
    "\t# Do t-tests to compare freqs of of any word that appears at least once in innov Dict\n",
    "\t# Get # of docs each corpus\n",
    "\tprint \"comparing full dictionaries\"\n",
    "\tnumDocsNonInnov = len(nonInnovDict['the'])\n",
    "\tnumDocsInnov = len(innovDict['the'])\n",
    "\tcompareDictResults = {}\n",
    "\tpowerCalc = TTestIndPower()\n",
    "\tfor k, v in innovDict.iteritems():\n",
    "\t\tinnovFreqs = innovDict[k]\n",
    "\t\tif k in nonInnovDict:\n",
    "\t\t\tnonInnovFreqs = nonInnovDict[k]\n",
    "\t\telse:\n",
    "\t\t\tnonInnovFreqs = [0.0] * numDocsNonInnov\n",
    "\t\t# Get max freqs\n",
    "\t\tmaxInnovFreq = max(innovFreqs)\n",
    "\t\tmaxNonInnovFreq = max(nonInnovFreqs)\n",
    "\t\t# T test, 2 sample unequal variance\n",
    "\t\tt_test = scipy.stats.ttest_ind(innovFreqs, nonInnovFreqs, axis=0, equal_var=False)\n",
    "\t\t# Power calculation, 2 sample t test with pooled std. get minimal detectable effect\n",
    "\t\tmde = powerCalc.solve_power(effect_size=None, nobs1=numDocsInnov, alpha=0.05, power=0.8, ratio= float(numDocsNonInnov)/numDocsInnov, alternative='two-sided') * pooledStd(innovFreqs, nonInnovFreqs)\n",
    "\t\tcompareDictResults[k] = {'meanInnovWordFreq': sum(innovFreqs) / len(innovFreqs), 'fracOfInnovDocsAppearing': sum(x > 0 for x in innovFreqs) / (0.0 + numDocsInnov), \n",
    "\t\t\t\t\t\t\t\t'meanNonInnovWordFreq': sum(nonInnovFreqs) / len(nonInnovFreqs), 'fracOfNonInnovDocsAppearing': sum(x > 0 for x in nonInnovFreqs) / (0.0 + numDocsNonInnov),\n",
    "\t\t\t\t\t\t\t\t'innovMinusNonInnovFreq': sum(innovFreqs) / len(innovFreqs) - sum(nonInnovFreqs) / len(nonInnovFreqs),\n",
    "\t\t\t\t\t\t\t\t'p_val': t_test[1], 'mde': mde, 'maxInnovFreq': maxInnovFreq, 'maxNonInnovFreq': maxNonInnovFreq, 'maxInnovMinusNonInnovFreq': maxInnovFreq - maxNonInnovFreq}\n",
    "\tsorted_compare = sorted(compareDictResults.items(), key=lambda t: -1 * t[1]['meanInnovWordFreq'])\n",
    "\treturn sorted_compare\n",
    "\n",
    "def pooledStd(arr1, arr2):\n",
    "\treturn math.sqrt(((len(arr1) - 1) * numpy.var(arr1) + (len(arr2) - 1) * numpy.var(arr2)) / (len(arr1) + len(arr2) - 2))\n",
    "\n",
    "def main():\n",
    "\tpcodes = [line.rstrip('\\n').split('\\t') for line in open('pcodes.txt')]\n",
    "\tcats = [x[0] for x in listOfWordLists]\n",
    "\t# COMPONENTS\n",
    "\tinnov_long_components, non_innov_long_components = featureExtractionHelper(\"FULL/components/non_innov_non_innov_TTL/baseline\", \"FULL/components/non_innov_non_innov_TTL/test\",\n",
    "\t\"FULL/components/innov/\", pcodes, isPdo = False, isRisk = False)\n",
    "\t# PDO\n",
    "\tinnov_long_PDO, non_innov_long_PDO = featureExtractionHelper(\"FULL/PDO/non_innov_non_innov_TTL/baseline\", \"FULL/PDO/non_innov_non_innov_TTL/test\",\n",
    "\t\t\"FULL/PDO/innov/\", pcodes, isPdo = True, isRisk = False)\n",
    "\t# Risk\n",
    "\tinnov_long_risk, non_innov_long_risk = featureExtractionHelper(\"FULL/risk/non_innov_non_innov_TTL/baseline\", \"FULL/risk/non_innov_non_innov_TTL/test\",\n",
    "\t\t\"FULL/risk/innov/\", pcodes, isPdo = False, isRisk = True)\n",
    "\n",
    "\tconfusion_matrix = []\n",
    "\taccuracies = []\n",
    "\tfor i in range(10):\n",
    "\t\t# Get full features\n",
    "\t\tinnov_final_features = getFullFeatures(innov_long_components, innov_long_PDO, innov_long_risk)#, innov_long_risk)\n",
    "\t\tnon_innov_final_features = getFullFeatures(non_innov_long_components, non_innov_long_PDO, non_innov_long_risk)#, non_innov_long_risk)\n",
    "\t\tcomputeFeatSignificance(innov_final_features, non_innov_final_features)\n",
    "\t\t# Model\n",
    "\t\tfrom sklearn.linear_model import LogisticRegression\n",
    "\t\tfeatures_train = innov_final_features[0:int(0.5*len(innov_final_features))] + non_innov_final_features[0:int(0.5*len(non_innov_final_features))] \n",
    "\t\tfeatures_test = innov_final_features[int(0.5*len(innov_final_features)):] + non_innov_final_features[int(0.5*len(non_innov_final_features)):] \n",
    "\t\tlabels_train = [1.0] * int(0.5*len(innov_final_features)) + [0.0] * int(0.5*len(non_innov_final_features))\n",
    "\t\tlabels_test = [1.0] * (len(innov_final_features) - int(0.5*len(innov_final_features))) + [0.0] * (len(non_innov_final_features) - int(0.5*len(non_innov_final_features)))\n",
    "\t\tmodel = LogisticRegression()\n",
    "\t\tmodel = model.fit(features_train, labels_train)\n",
    "\t\tprediction_lr = model.predict(features_test)\n",
    "\t\tconfusion_matrix.append(metrics.classification_report(labels_test, prediction_lr))\n",
    "\t\taccuracies.append(metrics.accuracy_score(prediction_lr, labels_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
